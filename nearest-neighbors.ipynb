{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRO\n",
    "\n",
    "* Basis for manifolds, spectral clustering, more\n",
    "* classification (discrete labels), regression (continuous labels)\n",
    "* Principle:\n",
    "   * Find predefined #samples closest to new point - predict label\n",
    "   * #samples: user-defined (kNN) or local density (radius based)\n",
    "* #samples possibly transformed into fast index struct [ball tree](http://scikit-learn.org/stable/modules/neighbors.html#ball-tree) or [KD tree](http://scikit-learn.org/stable/modules/neighbors.html#kd-tree)\n",
    "   \n",
    "[class API](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNSUPERVISED NN\n",
    "\n",
    "[API](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)\n",
    "\n",
    "* search options: `algorithm`='auto','ball_tree','kd_tree','brute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding nearest neighbors between 2 datasets\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.41421356],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.41421356]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build sparse graph showing connections betw neighboring points\n",
    "\n",
    "nbrs.kneighbors_graph(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KDTree\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "kdt = KDTree(X, leaf_size=30, metric='euclidean')\n",
    "kdt.query(X, k=2, return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BallTree\n",
    "from sklearn.neighbors import BallTree\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "kdt = BallTree(X, leaf_size=30, metric='euclidean')\n",
    "kdt.query(X, k=2, return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN CLASSIFICATION\n",
    "\n",
    "[API by KNN](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) |\n",
    "[API by radius](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier)\n",
    "\n",
    "* instance-based learning (doesn't build model, just stores training instances)\n",
    "* decision function = simple majority vote of NNs of each point\n",
    "* KNN more popular\n",
    "* Consider radius-basis when data not uniformly sampled\n",
    "* Radius function less effective in highD use cases\n",
    "* distance weightings can be set with `weights`='uniform'|'distance'\n",
    "\n",
    "[NN, iris dataset](plot_classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 0.66666667  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, y) \n",
    "print(neigh.predict([[1.1]]))\n",
    "print(neigh.predict_proba([[0.9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.5]]), array([[2]]))\n"
     ]
    }
   ],
   "source": [
    "# what point is closest to [1,1,1]?\n",
    "\n",
    "samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=1)\n",
    "neigh.fit(samples) \n",
    "\n",
    "print(neigh.kneighbors([[1., 1., 1.]])) \n",
    "\n",
    "# answer: element distance = 0.5, is the 3rd element in sample array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple point query\n",
    "X = [[0., 1., 0.], [1., 0., 1.]]\n",
    "neigh.kneighbors(X, return_distance=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.],\n",
       "       [ 0.,  1.,  1.],\n",
       "       [ 1.,  0.,  1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neighbor graph:\n",
    "X = [[0], [3], [1]]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "neigh.fit(X) \n",
    "\n",
    "A = neigh.kneighbors_graph(X)\n",
    "A.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN REGRESSION\n",
    "\n",
    "[API by KNN)](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) | [API by radius](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor)\n",
    "\n",
    "[demo](plot_regression.ipynb) | [\n",
    "face completion, multi-output](plot_multioutput_face_completion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALGORITHMS\n",
    "\n",
    "* BruteForce: scales as O(D x N^2) -- quickly becomes inefficient\n",
    "* [K-D tree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree): encodes aggregate distance, uses Cartesian axes as separators. complexity = O(D x N x log(N)). Less efficient as D increases.\n",
    "* [Ball tree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree): data partitioned in nesting hyperspheres w/ centroid & radius.\n",
    "\n",
    "*** Tradeoffs: ***\n",
    "\n",
    "* #samples\n",
    "* dimensionality (#features)\n",
    "* data structure (dimensionality, sparsity)\n",
    "* #neighbors requested (K)\n",
    "* #query points\n",
    "\n",
    "*** Leaf Size: ***\n",
    "\n",
    "* brute force > tree-based for small sample sizes\n",
    "* larger leaf size ==> faster tree construction (fewer nodes)\n",
    "* good compromise: leaf_size = 30 to optimize search time\n",
    "* as leaf_size goes up, memory rqmnts go up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEAREST CENTROID\n",
    "\n",
    "* represents each class by centroid of its membership\n",
    "\n",
    "### NEAREST CENTROID, SHUNKEN\n",
    "\n",
    "* uses `shrink_threshold` param\n",
    "* useful for removing noisy features\n",
    "\n",
    "[API](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) |\n",
    "[example](plot_nearest_centroid.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROXIMATE NEAREST NEIGHBORS\n",
    "\n",
    "*** For apps with K>50, AKA The 'Curse of Dimensionality' ***\n",
    "\n",
    "* When answers don't have to be exact\n",
    "\n",
    "[LSH Forest (API)](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LSHForest.html#sklearn.neighbors.LSHForest) |\n",
    "[demo: accuracy vs #candidates,#estimators](plot_approximate_nearest_neighbors_hyperparameters.ipynb) |\n",
    "[demo: query time vs #samples](plot_approximate_nearest_neighbors_scalability.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
