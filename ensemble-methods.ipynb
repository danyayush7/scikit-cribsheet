{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRO\n",
    "\n",
    "* combines predictions of multiple estimators with a learning algorithm\n",
    "* family 1: \"averaging methods\"\n",
    "   * (bagging, tree forests,...)\n",
    "* family 2: \"boosting methods\" = combine weak models to produce better one.\n",
    "   * (Adaboost, gradient tree boosting,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAGGING\n",
    "\n",
    "[API](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) | [demo: bias-variance analysis](plot_bias_variance.ipynb)\n",
    "\n",
    "* many variants, mostly re: how random subsets are selected\n",
    "* `max_samples`,`max_features` controls subset size\n",
    "* `bootstrap`,`bootstrap_features` controls with/without replacements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORESTS OF RANDOMIZED TREES\n",
    "\n",
    "[API (classifier)](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "\n",
    "[demo](plot_calibration_multiclass.ipynb)\n",
    "\n",
    "* two averaging algos: RandomForest, Extra-Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTEMELY RANDOMIZED TREES\n",
    "\n",
    "[Classifer](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) | [Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor) |\n",
    "[demo:plot forest iris](plot_forest_iris.ipynb)\n",
    "\n",
    "* Can adjust relative feature rank (depth) to asses importance compared to target predictability. Features at top of tree = contribute to decision surface on a larger fraction of inputs.\n",
    "\n",
    "[demo:pixel importances](plot_forest_importances_faces.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "    random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "scores.mean()                             \n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "scores.mean()                             \n",
    "\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "scores.mean() > 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST\n",
    "\n",
    "[Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) |\n",
    "[Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)\n",
    "\n",
    "* Fits a sequence of weak learners on repeatedly modified versions of the dataset.\n",
    "* Predictions are combined via weighted majority vote (or sum)\n",
    "* Each step: incorrectly predicted samples have their weights increased\n",
    "* Difficult-to-predict samples get ever-increasing influence,\n",
    "* so each subsequent weak learner = forced to concentrate on previous misses.\n",
    "\n",
    "[discrete vs real AdaBoost](plot_adaboost_hastie_10_2.ipynb) |\n",
    "[multiclass AdaBoosted DT](plot_adaboost_multiclass.ipynb) |\n",
    "[2class Adaboost](plot_adaboost_twoclass.ipynb) |\n",
    "[Adaboost regression](plot_adaboost_regression.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95996732026143794"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example, 100 weak learners\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, iris.data, iris.target)\n",
    "scores.mean()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADIENT TREE BOOSTING\n",
    "\n",
    "* Also: \"gradient boosted regression trees\" (GBRT)\n",
    "* Can be used for classification & regression\n",
    "* Support for adding estimators to already-fitted model via `warm_start`=True\n",
    "* Tree size controlled with `max_depth`=h and `max_leaf_nodes`=k\n",
    "\n",
    "[wiki](https://en.wikipedia.org/wiki/Gradient_boosting) | [classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) | [regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)\n",
    "\n",
    "[demo:regression](plot_gradient_boosting_regression.ipynb) |\n",
    "[demo:out-of-bag estimatess](plot_gradient_boosting_oob.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91300000000000003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: classifier with 100 decision stumps as weak learners\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0091548599603213"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: regressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(\n",
    "    n_samples=1200, \n",
    "    random_state=0, \n",
    "    noise=1.0)\n",
    "\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1,\n",
    "    max_depth=1, \n",
    "    random_state=0, \n",
    "    loss='ls').fit(X_train, y_train)\n",
    "\n",
    "mean_squared_error(y_test, est.predict(X_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example:effect of shrinkage & subsampling on model fit](plot_gradient_boosting_regularization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importances\n",
      " [ 0.11  0.1   0.11  0.1   0.09  0.11  0.09  0.1   0.1   0.09]\n",
      "pdp\n",
      " [[ 2.46643157  2.46643157  2.46643157  2.46643157  2.46643157  2.46643157\n",
      "   1.15418258  1.15418258  1.15418258  1.15418258  1.15418258  0.61847569\n",
      "   0.61847569  0.61847569  0.61847569  0.61847569  0.61847569  0.61847569\n",
      "   0.61847569 -0.03524098 -0.03524098 -0.03524098 -0.03524098 -0.03524098\n",
      "  -0.03524098 -0.03524098 -0.03524098 -0.03524098 -0.03524098 -0.03524098\n",
      "  -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365\n",
      "  -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365\n",
      "  -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365\n",
      "  -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365\n",
      "  -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365\n",
      "  -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.41817365\n",
      "  -0.41817365 -0.41817365 -0.41817365 -0.41817365 -0.03532577 -0.03532577\n",
      "  -0.03532577 -0.03532577 -0.03532577 -0.03532577 -0.03532577 -0.03532577\n",
      "  -0.03532577 -0.03532577  0.66315234  0.66315234  0.66315234  0.66315234\n",
      "   0.66315234  0.66315234  0.66315234  0.66315234  0.66315234  0.66315234\n",
      "   0.66315234  1.29415194  1.29415194  1.29415194  1.29415194  1.29415194\n",
      "   1.29415194  1.29415194  2.8904982   2.8904982 ]]\n",
      "axes\n",
      " [array([-1.62497054, -1.59201391, -1.55905727, -1.52610063, -1.49314399,\n",
      "       -1.46018736, -1.42723072, -1.39427408, -1.36131744, -1.32836081,\n",
      "       -1.29540417, -1.26244753, -1.22949089, -1.19653425, -1.16357762,\n",
      "       -1.13062098, -1.09766434, -1.0647077 , -1.03175107, -0.99879443,\n",
      "       -0.96583779, -0.93288115, -0.89992452, -0.86696788, -0.83401124,\n",
      "       -0.8010546 , -0.76809797, -0.73514133, -0.70218469, -0.66922805,\n",
      "       -0.63627142, -0.60331478, -0.57035814, -0.5374015 , -0.50444487,\n",
      "       -0.47148823, -0.43853159, -0.40557495, -0.37261831, -0.33966168,\n",
      "       -0.30670504, -0.2737484 , -0.24079176, -0.20783513, -0.17487849,\n",
      "       -0.14192185, -0.10896521, -0.07600858, -0.04305194, -0.0100953 ,\n",
      "        0.02286134,  0.05581797,  0.08877461,  0.12173125,  0.15468789,\n",
      "        0.18764452,  0.22060116,  0.2535578 ,  0.28651444,  0.31947107,\n",
      "        0.35242771,  0.38538435,  0.41834099,  0.45129762,  0.48425426,\n",
      "        0.5172109 ,  0.55016754,  0.58312418,  0.61608081,  0.64903745,\n",
      "        0.68199409,  0.71495073,  0.74790736,  0.780864  ,  0.81382064,\n",
      "        0.84677728,  0.87973391,  0.91269055,  0.94564719,  0.97860383,\n",
      "        1.01156046,  1.0445171 ,  1.07747374,  1.11043038,  1.14338701,\n",
      "        1.17634365,  1.20930029,  1.24225693,  1.27521356,  1.3081702 ,\n",
      "        1.34112684,  1.37408348,  1.40704011,  1.43999675,  1.47295339,\n",
      "        1.50591003,  1.53886667,  1.5718233 ,  1.60477994,  1.63773658])]\n"
     ]
    }
   ],
   "source": [
    "# how to \"see\" feature importance & partial dependence\n",
    "\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=1.0,\n",
    "    max_depth=1, \n",
    "    random_state=0).fit(X, y)\n",
    "\n",
    "print(\"importances\\n\",clf.feature_importances_ )\n",
    "\n",
    "features = [0, 1, (0, 1)]\n",
    "pdp,axes = partial_dependence(clf, [0], X=X)\n",
    "print(\"pdp\\n\",pdp)\n",
    "print(\"axes\\n\",axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[demo:plot partial dependence](plot_partial_dependence.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOTING CLASSIFIER\n",
    "\n",
    "* idea: combine conceptually different classifiers, use voting scheme to predict labels\n",
    "* hard voting: `voting`='hard' (majority)\n",
    "* soft voting: `voting`='hard' (argmax of sum of predicted probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.93 (+/- 0.05) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.05) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "# example, majority rule\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example, soft voting\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0,2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2,1,2])\n",
    "\n",
    "clf1 = clf1.fit(X,y)\n",
    "clf2 = clf2.fit(X,y)\n",
    "clf3 = clf3.fit(X,y)\n",
    "eclf = eclf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[demo](plot_voting_decision_regions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example, using VotingClassifier with GridSearch to tune parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', clf1), \n",
    "        ('rf', clf2), \n",
    "        ('gnb', clf3)], \n",
    "    voting='soft')\n",
    "\n",
    "params = {\n",
    "    'lr__C': [1.0, 100.0], \n",
    "    'rf__n_estimators': [20, 200],}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
