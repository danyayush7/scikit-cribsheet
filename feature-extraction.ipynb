{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "[API]() |\n",
    "[]() |\n",
    "[demo]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Loading Features from dicts](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer)\n",
    "\n",
    "* converts feature arrays (lists of dicts) to NumPy/SciPy representation, used by SciKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Fransisco', 'temperature': 18.},\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "vec.fit_transform(measurements).toarray()\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']\n"
     ]
    }
   ],
   "source": [
    "# also good for training sequence classifiers for NLP.\n",
    "# by extracting feature windows around a word of interest\n",
    "\n",
    "pos_window = [\n",
    "    {\n",
    "        'word-2': 'the',\n",
    "        'pos-2': 'DT',\n",
    "        'word-1': 'cat',\n",
    "        'pos-1': 'NN',\n",
    "        'word+1': 'on',\n",
    "        'pos+1': 'PP',\n",
    "    },\n",
    "    # in a real application one would extract many such dictionaries\n",
    "]\n",
    "\n",
    "# vectorize into sparse 2D array\n",
    "vec = DictVectorizer()\n",
    "pos_vectorized = vec.fit_transform(pos_window)\n",
    "print(pos_vectorized)               \n",
    "\n",
    "pos_vectorized.toarray()\n",
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feature Hashing](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) |\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Feature_hashing) |\n",
    "[demo](hashing_vs_dict_vectorizer.ipynb)\n",
    "\n",
    "* instead of building a hash table of training features,\n",
    "* apply hash funct to features = find their column index in sample matrices directly?\n",
    "* advantage: better speed, better memory usage\n",
    "* disadvtge: inspectability\n",
    "* signed 32b hash used to resolve collisions between unrelated features\n",
    "* so max #features supported = 2^31-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: word-level NLP task\n",
    "# features extracted from (token, part_of_speech) pairs\n",
    "def token_features(token, part_of_speech):\n",
    "    if token.isdigit():\n",
    "        yield \"numeric\"\n",
    "    else:\n",
    "        yield \"token={}\".format(token.lower())\n",
    "        yield \"token,pos={},{}\".format(token, part_of_speech)\n",
    "    if token[0].isupper():\n",
    "        yield \"uppercase_initial\"\n",
    "    if token.isupper():\n",
    "        yield \"all_uppercase\"\n",
    "    yield \"pos={}\".format(part_of_speech)\n",
    "\n",
    "#raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\n",
    "#hasher = FeatureHasher(input_type='string')\n",
    "#X = hasher.transform(raw_X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feature Extraction (Text)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) |\n",
    "[demo](topics_extraction_with_nmf_lda.ipynb)\n",
    "\n",
    "* capability: tokenizing (strings => id), counting #tokens, normalizing\n",
    "* resulting matrix will be sparse, usually >99%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer usage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'Hello Hello Hello Hello Hello',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus) # tokenize the corpus\n",
    "X   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 5, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default config: tokenize any words of 2 letters or larger\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (\n",
    "    ['this', 'is', 'text', 'document', 'to', 'analyze'])\n",
    "\n",
    "vectorizer.get_feature_names() == (\n",
    "    ['and', 'document', 'first', 'is', 'one',\n",
    "     'second', 'the', 'third', 'this'])\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converse mapping from feature name to column index\n",
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words not seen in corpus will be ignored in future calls to transformer:\n",
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preserving local ordering info\n",
    "bigram_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r'\\b\\w+\\b', \n",
    "    min_df=1)\n",
    "\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "\n",
    "analyze('Bi-grams are cool!') == (\n",
    "    ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab extracted by vectorizer\n",
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is \"is this\" in the corpus?\n",
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "X_2[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [API (TFIDF: transformer)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)\n",
    "\n",
    "### [API (TFIDF: vectorizer)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "* used to re-weight feature counts to floating point - for usage by a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization done by a Transformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<6x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 9 stored elements in Compressed Sparse Row format>,\n",
       " array([[ 0.81940995,  0.        ,  0.57320793],\n",
       "        [ 1.        ,  0.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ,  0.        ],\n",
       "        [ 0.47330339,  0.88089948,  0.        ],\n",
       "        [ 0.58149261,  0.        ,  0.81355169]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf, tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combines CountVectorizer+TfidfTransformer in single model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Text Files\n",
    "\n",
    "* text = characters; files = bytes = characters+encoding\n",
    "* Python rqmnt: bytes must be decoded to Unicode charset\n",
    "* Common encodings: ASCII, Latin-1, KOI8-R, UTF-8, UTF-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n",
      "{'flügeln': 4, 'gegrăźă': 6, 'mein': 12, 'sind': 16, 'herzliebchen': 9, 'dich': 3, 'fort': 5, 'sei': 15, 'holdselig': 10, 'gesanges': 8, 'deine': 1, 'sauerkraut': 14, 'ich': 11, 'auf': 0, 'mir': 13, 'des': 2, 'gerüche': 7, 'trag': 17}\n"
     ]
    }
   ],
   "source": [
    "#chardet learns decodings, vectorizes texts, prints learned vocab.\n",
    "import chardet    \n",
    "text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n",
    "text2 = b\"holdselig sind deine Ger\\xfcche\"\n",
    "text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n",
    "decoded = [x.decode(chardet.detect(x)['encoding'])\n",
    "           for x in (text1, text2, text3)]        \n",
    "v = CountVectorizer().fit(decoded).vocabulary_    \n",
    "for term in v: print(v)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Limitations\n",
    "\n",
    "* Bag-of-Words == collection of unigrams. Can't capture phrases or multi-word expressions.\n",
    "* N-grams can help. Ex: n=2 grams == pairs of consecutive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using 'char_wb analyzer\n",
    "ngram_vectorizer = CountVectorizer(\n",
    "    analyzer='char_wb', \n",
    "    ngram_range=(2, 2), \n",
    "    min_df=1)\n",
    "counts = ngram_vectorizer.fit_transform(\n",
    "    ['words', 'wprds'])\n",
    "ngram_vectorizer.get_feature_names() == (\n",
    "    [' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])\n",
    "\n",
    "counts.toarray().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using 'char' analyzer\n",
    "ngram_vectorizer1 = CountVectorizer(\n",
    "    analyzer='char_wb', \n",
    "    ngram_range=(5, 5), \n",
    "    min_df=1)\n",
    "\n",
    "ngram_vectorizer1.fit_transform(\n",
    "    ['jumpy fox'])\n",
    "\n",
    "ngram_vectorizer1.get_feature_names() == (\n",
    "    [' fox ', ' jump', 'jumpy', 'umpy '])\n",
    "\n",
    "\n",
    "ngram_vectorizer2 = CountVectorizer(\n",
    "    analyzer='char', \n",
    "    ngram_range=(5, 5), \n",
    "    min_df=1)\n",
    "\n",
    "ngram_vectorizer2.fit_transform(\n",
    "    ['jumpy fox'])\n",
    "                      \n",
    "ngram_vectorizer2.get_feature_names() == (\n",
    "    ['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Large Text Corpus with Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing a func to the constructor\n",
    "def my_tokenizer(s):\n",
    "    return s.split()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n",
    "vectorizer.build_analyzer()(u\"Some... punctuation!\") == (\n",
    "    ['some...', 'punctuation!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Token Analysis using NLTK\n",
    "\n",
    "[NLTK](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<__main__.LemmaTokenizer object at 0x7fbb71eea588>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=LemmaTokenizer()) \n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Extraction\n",
    "\n",
    "[extract 2D image patches](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d) |\n",
    "[reconstruct_from_patches_2d](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d.html#sklearn.feature_extraction.image.reconstruct_from_patches_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 3)\n",
      "(9, 2, 2, 3)\n",
      "[[15 18]\n",
      " [27 30]]\n"
     ]
    }
   ],
   "source": [
    "# generate 4x4 pixel image, RGB format\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "\n",
    "one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\n",
    "one_image[:, :, 0]  # R channel of a fake RGB picture\n",
    "\n",
    "patches = image.extract_patches_2d(\n",
    "    one_image, \n",
    "    (2, 2), \n",
    "    max_patches=2,\n",
    "    random_state=0)\n",
    "\n",
    "print(patches.shape)\n",
    "\n",
    "patches[:, :, :, 0]\n",
    "\n",
    "patches = image.extract_patches_2d(\n",
    "    one_image, (2, 2))\n",
    "\n",
    "print(patches.shape)\n",
    "print(patches[4, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2, 2, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempt image reconstruction from patches\n",
    "# by averaging on overlaps\n",
    "reconstructed = image.reconstruct_from_patches_2d(\n",
    "    patches, \n",
    "    (4, 4, 3))\n",
    "\n",
    "np.testing.assert_array_equal(\n",
    "    one_image, reconstructed)\n",
    "\n",
    "five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\n",
    "patches = image.PatchExtractor((2, 2)).transform(five_images)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image connectivity graph\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
