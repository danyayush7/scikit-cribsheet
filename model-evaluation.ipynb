{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "* 3 approaches to eval prediction quality:\n",
    "   * Estimator score (\"score_\" method, provided by estimator tool)\n",
    "   * Scoring parameter (provided by cross-validation tools)\n",
    "   * Metric function (metrics module)\n",
    "\n",
    "Also: [dummy estimators](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring parameters\n",
    "\n",
    "* convention: higher scores > lower scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07475338 -0.16911634 -0.0698804 ]\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "clf = svm.SVC(\n",
    "    probability=True, \n",
    "    random_state=0)\n",
    "\n",
    "print(cross_val_score(\n",
    "    clf, X, y, \n",
    "    scoring='neg_log_loss'))\n",
    "\n",
    "#model = svm.SVC()\n",
    "#cross_val_score(\n",
    "#    model, X, y, \n",
    "#    scoring='wrong_choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - defining score strategy from metric functions\n",
    "\n",
    "* Use case #1: wrap existing metric from library with non-default parameter values, ex: `beta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#example - defining score strategy from metric functions\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(\n",
    "    fbeta_score, \n",
    "    beta=2)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "grid = GridSearchCV(\n",
    "    LinearSVC(), \n",
    "    param_grid={'C': [1, 10]}, \n",
    "    scoring=ftwo_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use case #2: build custom scorer from python function\n",
    "* using [make_scorer](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.69314718056\n",
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    diff = np.abs(ground_truth - predictions).max()\n",
    "    return np.log(1 + diff)\n",
    "\n",
    "# loss_func will negate the return value of my_custom_loss_func,\n",
    "#  which will be np.log(2), 0.693, given the values for ground_truth\n",
    "#  and predictions defined below.\n",
    "\n",
    "loss  = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "ground_truth = [[1, 1]]\n",
    "predictions  = [0, 1]\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(ground_truth, predictions)\n",
    "\n",
    "print(loss(clf,ground_truth, predictions))\n",
    "\n",
    "print(score(clf,ground_truth, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### accuracy score\n",
    "\n",
    "* either the fraction or count of correct predictions\n",
    "\n",
    "[demo](plot_permutation_test_for_classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "\n",
    "print(accuracy_score(\n",
    "        y_true, y_pred))\n",
    "print(accuracy_score(\n",
    "        y_true, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cohen's Kappa](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score) | [Wikipedia](https://en.wikipedia.org/wiki/Cohen%27s_kappa)\n",
    "\n",
    "* Intended to compare labelings by human annotators, not a classifier vs ground truth\n",
    "* range [-1,+1]; >0.8 generally considered good; 0 = basically random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4285714285714286"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "cohen_kappa_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Confusion Matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) | [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) | [Demo](plot_confusion_matrix.ipynb)\n",
    "\n",
    "* definition: data(i,j) in confusion matrix = #observations actually in group i, but predicted to be in group j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix:\n",
      " [[2 0 0]\n",
      " [0 0 1]\n",
      " [1 0 2]]\n",
      "counts:\n",
      " 2 1 2 3\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "print(\"matrix:\\n\",confusion_matrix(\n",
    "        y_true, y_pred))\n",
    "\n",
    "# To get counts of (true/false)(positives/negatives)\n",
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(\"counts:\\n\",tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Classification Report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)\n",
    "\n",
    "* returns text report with main metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      1.00      0.80         2\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "\n",
    "print(classification_report(\n",
    "        y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hamming loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) | [Wikipedia](https://en.wikipedia.org/wiki/Hamming_distance)\n",
    "\n",
    "* Finds hamming distance between two sets of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# hamming loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "\n",
    "print(hamming_loss(\n",
    "        y_true, y_pred))\n",
    "\n",
    "# multilabel use case, binary label indicators\n",
    "\n",
    "print(hamming_loss(\n",
    "        np.array([[0, 1], [1, 1]]), \n",
    "        np.zeros((2, 2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Jaccard score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score) | [Wikipedia](https://en.wikipedia.org/wiki/Jaccard_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# jaccard score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "print(jaccard_similarity_score(\n",
    "        y_true, y_pred))\n",
    "\n",
    "print(jaccard_similarity_score(\n",
    "        y_true, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision), [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall), [F1-score](https://en.wikipedia.org/wiki/F1_score)\n",
    "\n",
    "* precision: ability to avoid labeling a negative sample as positive\n",
    "* recall: ability to find all positive samples\n",
    "* F-measure: weighted harmonic mean of precision & recall (best=1, worst=0)\n",
    "\n",
    "[precision/recall curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve) |\n",
    "[avg precision score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) |\n",
    "[demo](plot_precision_recall.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score:\n",
      " 1.0\n",
      "recall score:\n",
      " 0.5\n",
      "f1 score:\n",
      " 0.666666666667\n",
      "fbeta score, beta=0.5\n",
      " 0.833333333333\n",
      "fbeta score, beta=1.0\n",
      " 0.666666666667\n",
      "fbeta score, beta=2.0\n",
      " 0.555555555556\n",
      "precision recall fscore support\n",
      " (array([ 0.66666667,  1.        ]), array([ 1. ,  0.5]), array([ 0.71428571,  0.83333333]), array([2, 2]))\n",
      "\n",
      "\n",
      "precision:\n",
      " [ 0.66666667  0.5         1.          1.        ]\n",
      "recall:\n",
      " [ 1.   0.5  0.5  0. ]\n",
      "threshold:\n",
      " [ 0.35  0.4   0.8 ]\n",
      "avg precision score:\n",
      " 0.791666666667\n"
     ]
    }
   ],
   "source": [
    "# binary classification examples\n",
    "\n",
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "\n",
    "print(\"precision score:\\n\",metrics.precision_score(\n",
    "        y_true, y_pred))\n",
    "print(\"recall score:\\n\",metrics.recall_score(\n",
    "        y_true, y_pred))\n",
    "print(\"f1 score:\\n\",metrics.f1_score(\n",
    "        y_true, y_pred))\n",
    "print(\"fbeta score, beta=0.5\\n\",metrics.fbeta_score(\n",
    "        y_true, y_pred, beta=0.5))\n",
    "print(\"fbeta score, beta=1.0\\n\",metrics.fbeta_score(\n",
    "        y_true, y_pred, beta=1))\n",
    "print(\"fbeta score, beta=2.0\\n\",metrics.fbeta_score(\n",
    "        y_true, y_pred, beta=2))\n",
    "print(\"precision recall fscore support\\n\",metrics.precision_recall_fscore_support(\n",
    "        y_true, y_pred, beta=0.5))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "precision, recall, threshold = precision_recall_curve(\n",
    "    y_true, y_scores)\n",
    "print(\"\\n\")\n",
    "print(\"precision:\\n\",precision)\n",
    "print(\"recall:\\n\",recall)\n",
    "print(\"threshold:\\n\",threshold)\n",
    "print(\"avg precision score:\\n\",average_precision_score(y_true, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass / multilabel classification metrics\n",
    "\n",
    "* precision, recall, f-measures can be applied to each label independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:\n",
      " 0.222222222222\n",
      "recall:\n",
      " 0.333333333333\n",
      "f1 score:\n",
      " 0.266666666667\n",
      "fbeta score, avg=macro:\n",
      " 0.238095238095\n",
      "fbeta score, avg=none:\n",
      " (array([ 0.66666667,  0.        ,  0.        ]), array([ 1.,  0.,  0.]), array([ 0.71428571,  0.        ,  0.        ]), array([2, 2, 2]))\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from sklearn import metrics\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "\n",
    "print(\"precision:\\n\",metrics.precision_score(\n",
    "        y_true, y_pred, average='macro'))\n",
    "print(\"recall:\\n\",metrics.recall_score(\n",
    "        y_true, y_pred, average='micro'))\n",
    "print(\"f1 score:\\n\",metrics.f1_score(\n",
    "        y_true, y_pred, average='weighted')) \n",
    "print(\"fbeta score, avg=macro:\\n\",metrics.fbeta_score(\n",
    "        y_true, y_pred, average='macro', beta=0.5))\n",
    "print(\"fbeta score, avg=none:\\n\",metrics.precision_recall_fscore_support(\n",
    "        y_true, y_pred, beta=0.5, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hinge loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss) | [Wikipedia](https://en.wikipedia.org/wiki/Hinge_loss)\n",
    "\n",
    "* returns avg distance between model and actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision:\n",
      " [-2.18177944  2.36355888  0.09088972]\n",
      "loss:\n",
      " 0.303036760385\n"
     ]
    }
   ],
   "source": [
    "# hinge loss, SVM classifier, binary class problem\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import hinge_loss\n",
    "\n",
    "X = [[0], [1]]\n",
    "y = [-1, 1]\n",
    "\n",
    "est = svm.LinearSVC(random_state=0)\n",
    "est.fit(X, y)\n",
    "\n",
    "pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
    "\n",
    "print(\"decision:\\n\",pred_decision)\n",
    "print(\"loss:\\n\",hinge_loss(\n",
    "        [-1, 1, 1], \n",
    "        pred_decision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision:\n",
      " [[ 1.27271735  0.03419428 -0.68377145 -1.4016886 ]\n",
      " [-1.45454347 -0.58117471 -0.37609483 -0.17096867]\n",
      " [-2.36363041 -0.78629771 -0.27353595  0.23927131]]\n",
      "loss:\n",
      " 0.564106298623\n"
     ]
    }
   ],
   "source": [
    "# hinge loss, SVM classifier, multiclass problem\n",
    "\n",
    "X = np.array([[0], [1], [2], [3]])\n",
    "Y = np.array([0, 1, 2, 3])\n",
    "\n",
    "labels = np.array([0, 1, 2, 3])\n",
    "est = svm.LinearSVC()\n",
    "est.fit(X, Y)\n",
    "\n",
    "pred_decision = est.decision_function([[-1], [2], [3]])\n",
    "print(\"decision:\\n\",pred_decision)\n",
    "\n",
    "y_true = [0, 2, 3]\n",
    "print(\"loss:\\n\",hinge_loss(\n",
    "        y_true, pred_decision, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Log loss (logistic regression loss, cross-entropy loss)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss) | [demo](plot_calibration_multiclass.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      " 0.173807336691\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "from sklearn.metrics import log_loss\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "\n",
    "print(\"loss:\\n\",log_loss(\n",
    "        y_true, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Matthews correlation coefficient (MCC)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef) | [wiki](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
    "\n",
    "* a measure of binary (2-class) classification model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC:\n",
      " -0.333333333333\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "y_true = [+1, +1, +1, -1]\n",
    "y_pred = [+1, -1, +1, +1]\n",
    "print(\"MCC:\\n\",matthews_corrcoef(\n",
    "        y_true, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Receiver Operating Characteristic (ROC)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) | [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "* illustrates binary classifier quality as its discrimination threshold is varied. \n",
    "* fraction of true positives/positives (TPR, also called sensitivity) vs false positives/negatives (FPR)\n",
    "\n",
    "[demo](plot_roc.ipynb) | \n",
    "[demo w/ CV](plot_roc_crossval.ipynb) |\n",
    "[demo - species modeling](plot_species_distribution_modeling.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:\n",
      " [ 0.   0.5  0.5  1. ]\n",
      "TPR:\n",
      " [ 0.5  0.5  1.   1. ]\n",
      "thresholds:\n",
      " [ 0.8   0.4   0.35  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y = np.array([1, 1, 2, 2])\n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n",
    "\n",
    "print(\"FPR:\\n\",fpr)\n",
    "print(\"TPR:\\n\",tpr)\n",
    "print(\"thresholds:\\n\",thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Zero One Loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss) | [demo](plot_adaboost_hastie_10_2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      " 0.25\n",
      "loss, not normalized:\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "# zero one loss\n",
    "from sklearn.metrics import zero_one_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "\n",
    "print(\"loss:\\n\",zero_one_loss(\n",
    "        y_true, y_pred))\n",
    "print(\"loss, not normalized:\\n\",zero_one_loss(\n",
    "        y_true, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Brier score loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) | [Wikipedia](https://en.wikipedia.org/wiki/Brier_score) | [demo](plot_calibration_curve.ipynb)\n",
    "\n",
    "* returns score mean square difference between actual outcome (0,1) & predicated probability of possible outcome (0-1)\n",
    "* lower score = more accurate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055\n",
      "0.055\n",
      "0.055\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# brier score loss\n",
    "import numpy as np\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "y_true = np.array([0, 1, 1, 0])\n",
    "y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
    "y_prob = np.array([0.1, 0.9, 0.8, 0.4])\n",
    "y_pred = np.array([0, 1, 1, 0])\n",
    "\n",
    "print(brier_score_loss(\n",
    "        y_true, y_prob))\n",
    "\n",
    "print(brier_score_loss(\n",
    "        y_true, 1-y_prob, pos_label=0))\n",
    "\n",
    "print(brier_score_loss(\n",
    "        y_true_categorical, y_prob, pos_label=\"ham\"))\n",
    "\n",
    "print(brier_score_loss(\n",
    "        y_true, y_prob > 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Label Ranking metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Coverage Error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.coverage_error.html#sklearn.metrics.coverage_error)\n",
    "\n",
    "* finds avg #labels required in final prediction so all true labels are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "import numpy as np\n",
    "from sklearn.metrics import coverage_error\n",
    "\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "coverage_error(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Label Ranking Avg Precision (LRAP)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html#sklearn.metrics.label_ranking_average_precision_score)\n",
    "\n",
    "* returns average over each ground truth label assigned to each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41666666666666663"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example LRAP\n",
    "import numpy as np\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "label_ranking_average_precision_score(y_true, y_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Ranking Loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_loss.html#sklearn.metrics.label_ranking_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "import numpy as np\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "print(label_ranking_loss(\n",
    "        y_true, y_score))\n",
    "\n",
    "# With the following prediction, we have perfect and minimal loss\n",
    "y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])\n",
    "print(label_ranking_loss(\n",
    "        y_true, y_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Explained Variance Score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score)\n",
    "\n",
    "* Optimum = 1.0, worst case = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score #1:\n",
      " 0.957173447537\n",
      "score #2 (multiout=raw):\n",
      " [ 0.96774194  1.        ]\n",
      "score #2 (multiout=given):\n",
      " 0.990322580645\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "print(\"score #1:\\n\",explained_variance_score(\n",
    "        y_true, y_pred)) \n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0,   2], [-1, 2], [8, -5]]\n",
    "\n",
    "print(\"score #2 (multiout=raw):\\n\",explained_variance_score(\n",
    "        y_true, y_pred, \n",
    "        multioutput='raw_values'))\n",
    "\n",
    "print(\"score #2 (multiout=given):\\n\",explained_variance_score(\n",
    "    y_true, y_pred, multioutput=[0.3, 0.7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Mean Absolute Error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error) | [wiki](https://en.wikipedia.org/wiki/Mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.75\n",
      "[ 0.5  1. ]\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(mean_absolute_error(\n",
    "        y_true, y_pred))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(mean_absolute_error(\n",
    "        y_true, y_pred))\n",
    "\n",
    "print(mean_absolute_error(\n",
    "        y_true, y_pred, multioutput='raw_values'))\n",
    "\n",
    "print(mean_absolute_error(\n",
    "        y_true, y_pred, multioutput=[0.3, 0.7]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Mean Squared Error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) | [wiki](https://en.wikipedia.org/wiki/Mean_squared_error) | [demo: GBR](plot_gradient_boosting_regression.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "0.708333333333\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(mean_squared_error(\n",
    "        y_true, y_pred))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(mean_squared_error(\n",
    "        y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Mean Absolute Error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error)\n",
    "\n",
    "* Robust to outliers\n",
    "* No multioutput support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "from sklearn.metrics import median_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "median_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [R^2 score (coefficient of determination)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) | [wiki](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "* Returns measure of how well future samples will be predicted by current model.\n",
    "* Best case = 1.0, can be <0.0\n",
    "* 0.0 = always predicts expected value (disregarding input features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948608137045\n",
      "0.938256658596\n",
      "0.936800526662\n",
      "[ 0.96543779  0.90816327]\n",
      "0.92534562212\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from sklearn.metrics import r2_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(r2_score(\n",
    "        y_true, y_pred))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(r2_score(\n",
    "        y_true, y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(r2_score(\n",
    "        y_true, y_pred, multioutput='uniform_average'))\n",
    "\n",
    "print(r2_score(\n",
    "        y_true, y_pred, multioutput='raw_values'))\n",
    "\n",
    "print(r2_score(\n",
    "        y_true, y_pred, multioutput=[0.3, 0.7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering metrics\n",
    "\n",
    "[instance clustering](clustering.ipynb) |\n",
    "[biclustering](biclustering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy estimators\n",
    "\n",
    "[Dummy classifiers](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier) | [Dummy regressors](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor)\n",
    "\n",
    "* use case: supervised learning - comparing an estimator against simple rules of thumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create unbalanced dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "y[y != 1] = -1\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.63157894736842102, 0.57894736842105265)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare accuracy of SVC & most_frequent\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf1 = SVC(\n",
    "    kernel='linear', \n",
    "    C=1).fit(X_train, y_train)\n",
    "\n",
    "clf2 = DummyClassifier(\n",
    "    strategy='most_frequent',\n",
    "    random_state=0).fit(X_train, y_train)\n",
    "\n",
    "clf1.score(X_test, y_test) ,clf2.score(X_test, y_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC doesn't do much better than dummy classifier. Now chg kernel.\n",
    "clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy boosted to near 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
